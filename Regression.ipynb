{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. What is Simple Linear Regression?**\n",
        "A statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) using a straight line:\n",
        "**Y = mX + c**\n",
        "\n",
        "---\n",
        "\n",
        "**2. What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "* Linearity\n",
        "* Independence of errors\n",
        "* Homoscedasticity (constant variance of errors)\n",
        "* Normality of residuals\n",
        "* No significant outliers\n",
        "\n",
        "---\n",
        "\n",
        "**3. What does the coefficient m represent in the equation Y = mX + c?**\n",
        "It is the **slope**, indicating how much Y changes for a one-unit increase in X.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What does the intercept c represent in the equation Y = mX + c?**\n",
        "It is the value of Y when X is zero; the point where the line intersects the Y-axis.\n",
        "\n",
        "---\n",
        "\n",
        "**5. How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sum{(X_i - \\bar{X})^2}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**6. What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "To find the best-fit line by minimizing the sum of squared residuals (errors between actual and predicted values).\n",
        "\n",
        "---\n",
        "\n",
        "**7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "R² shows the proportion of variance in Y explained by X.\n",
        "For example, **R² = 0.80** means 80% of the variation in Y is explained by X.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**8. What is Multiple Linear Regression?**\n",
        "A regression model involving **more than one** independent variable to predict a dependent variable:\n",
        "**Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ**\n",
        "\n",
        "---\n",
        "\n",
        "**9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "Simple uses **one** predictor; multiple uses **two or more** predictors.\n",
        "\n",
        "---\n",
        "\n",
        "**10. What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "* Linearity\n",
        "* No multicollinearity\n",
        "* Homoscedasticity\n",
        "* Independence\n",
        "* Normality of residuals\n",
        "\n",
        "---\n",
        "\n",
        "**11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "It means the **variance of errors is not constant**. It leads to inefficient estimates and incorrect significance tests.\n",
        "\n",
        "---\n",
        "\n",
        "**12. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "* Remove correlated predictors\n",
        "* Use PCA (Principal Component Analysis)\n",
        "* Apply **Ridge or Lasso Regression**\n",
        "\n",
        "---\n",
        "\n",
        "**13. What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "* **Label Encoding**\n",
        "* **One-Hot Encoding**\n",
        "* **Dummy Variables**\n",
        "\n",
        "---\n",
        "\n",
        "**14. What is the role of interaction terms in Multiple Linear Regression?**\n",
        "They model the combined effect of two or more variables, capturing relationships like:\n",
        "**Y = b₀ + b₁X₁ + b₂X₂ + b₃(X₁\\*X₂)**\n",
        "\n",
        "---\n",
        "\n",
        "**15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "In simple regression: value of Y when X = 0.\n",
        "In multiple regression: Y when **all X’s = 0**, which may not be meaningful.\n",
        "\n",
        "---\n",
        "\n",
        "**16. What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "It shows the rate of change in Y for a unit increase in X. A significant slope means X has predictive power.\n",
        "\n",
        "---\n",
        "\n",
        "**17. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "It gives the **baseline value** of Y when all predictors are zero, helping understand the offset in predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**18. What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "* R² always increases with more variables\n",
        "* It doesn’t detect overfitting\n",
        "* Doesn’t assess prediction error\n",
        "\n",
        "---\n",
        "\n",
        "**19. How would you interpret a large standard error for a regression coefficient?**\n",
        "It suggests **low precision** in estimating that coefficient; predictor may be weak or noisy.\n",
        "\n",
        "---\n",
        "\n",
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "Look for **funnel shapes** in residual vs. predicted plots. It can invalidate p-values and lead to biased models.\n",
        "\n",
        "---\n",
        "\n",
        "**21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "It suggests **overfitting**; added variables do not meaningfully improve the model.\n",
        "\n",
        "---\n",
        "\n",
        "**22. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "To ensure equal importance across features and improve convergence in optimization (especially for regularized models).\n",
        "\n",
        "---\n",
        "\n",
        "**23. What is polynomial regression?**\n",
        "A type of regression where the relationship between the independent variable and the dependent variable is modeled as an **nth-degree polynomial**.\n",
        "\n",
        "---\n",
        "\n",
        "**24. How does polynomial regression differ from linear regression?**\n",
        "Polynomial regression can model **non-linear** curves, while linear regression fits a **straight line**.\n",
        "\n",
        "---\n",
        "\n",
        "**25. When is polynomial regression used?**\n",
        "When the data shows a **curved or non-linear** relationship between variables.\n",
        "\n",
        "---\n",
        "\n",
        "**26. What is the general equation for polynomial regression?**\n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + ... + b_nX^n\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**27. Can polynomial regression be applied to multiple variables?**\n",
        "Yes, it's called **Multiple Polynomial Regression**, using terms like X₁², X₁X₂, etc.\n",
        "\n",
        "---\n",
        "\n",
        "**28. What are the limitations of polynomial regression?**\n",
        "\n",
        "* Risk of **overfitting** with high degrees\n",
        "* **Poor extrapolation** beyond training data\n",
        "* **Hard to interpret** higher-degree terms\n",
        "\n",
        "---\n",
        "\n",
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "* **Cross-validation**\n",
        "* **Adjusted R²**\n",
        "* **AIC/BIC scores**\n",
        "* **Residual analysis**\n",
        "\n",
        "---\n",
        "\n",
        "**30. Why is visualization important in polynomial regression?**\n",
        "It helps understand the **fit of the curve**, detect **overfitting**, and communicate model behavior.\n",
        "\n",
        "---\n",
        "\n",
        "**31. How is polynomial regression implemented in Python?**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures  \n",
        "from sklearn.linear_model import LinearRegression  \n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example:\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ilWtM6FnZDgD"
      }
    }
  ]
}